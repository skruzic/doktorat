\chapter{INTRODUCTION}
\label{chap:Intro}

Robotic manipulators have become ubiquitous over the past decades, with different areas of application in automated industrial processes and beyond. Thus, in recent years we have increasingly found them in applications to help people (termed \emph{assistive robotics}) and in various variants of mobile manipulators in commercial applications such as in warehouses and stores. In these cases, the mobile manipulator is defined as a robotic manipulator that is mounted (attached) to the mobile robotic base and can thus move in two-dimensional space.

Regardless of the area of use, robotic manipulators perform various tasks and usually work in an open loop, i.e., they are programmed to perform predefined trajectories (movements) to complete a given task and have no sensors to give feedback (and thus close the control loop), and it is not possible to know if there was a problem performing the task. However, in applications where the position/configuration of the robot alone cannot guarantee the successful execution of a given task, this approach is inappropriate. These are usually tasks that require physical interaction between the robot and the environment and in the collaboration of man and robot or several different robots (which is often the case in Industry 4.0). To be able to obtain data on that interaction, it needs to be measured. The interaction in the physical sense consists of forces and torques, which can be measured by placing a sensor on the robot to measure the forces and torques. Furthermore, the robot usually interacts with the environment by touching the environment with its tip. Thus, the force and torque sensor is most often placed on the robot tip.

Sometimes it is inconvenient or impossible to place a force and torque sensor on the base of the robot arm (especially emphasised in smaller robots, which are usually intended for education) due to the relatively large mass of such sensors, which significantly reduces the payload of the robot. Therefore, it is necessary to find another way to reliably (and indirectly) measure these values. The logical possibility that arises is to place force-torque sensors under the robot base because this does not affect the payload and allows such measured values and robot models (i.e., geometry and physics) to estimate force values at the robot's tip (or any other robot joint, if necessary).

The usual way to solve this problem involves using a dynamic model of a robot, which must be accurate because otherwise, the obtained results would not be satisfactory. However, the dynamic model of the robot depends on the inertial parameters of the robot segments (mass, shape, dimensions). If these parameters are incorrect, the error accumulates over time by integrating the differential equations of robot dynamics, which makes the obtained results unsatisfactory. The motivation for researching this area lies in applying deep learning methods and neural networks to solve this problem. Namely, neural networks can generalise, so a robot model is unnecessary because it can be learned from data. Another advantage of this approach is that the robot model, once learnt, can be used indefinitely in real-time even when computing resources are relatively limited, while numerical methods for solving differential / integral equations of robot dynamics on a computer are much slower (and contain certain approximations), these may be inappropriate for real-time execution.

In addition to all the above, robotic manipulators have one additional significant disadvantage: they are attached at a fixed place and consequently have a relatively narrow and fixed working environment. Therefore, a logical step toward significantly increasing the working environment may be to place a robotic manipulator on a mobile robotic base forming a mobile robotic manipulator. Such mobile manipulators potentially have numerous applications in modern industry, logistics and assistive robotics.

However, the mobility of the robotic manipulator opens up new issues that need to be addressed and related to autonomous navigation. In general, navigation in a broader sense involves autonomous driving to a given destination (or following a given trajectory) and avoiding obstacles (static and dynamic). However, avoiding obstacles cannot always be fully reconciled with navigating to a destination (sometimes they have the opposite effect -- avoiding obstacles can, in some situations, contribute to moving away from a destination or a given route). Therefore, a navigation approach should be developed in which it will be possible to separate these two components into two separate controllers, each for one type of behaviour (driving to the goal and avoiding obstacles), and find a way to combine these two (primary) behaviours to complex behaviour occurred. Furthermore, this approach should allow the navigation and obstacle avoidance logic to be separated, allowing each to be freely / easily changed without affecting the latter.

\newpage